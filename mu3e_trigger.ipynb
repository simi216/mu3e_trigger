{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4436aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dd0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"mu3e_trigger_data\"\n",
    "SIGNAL_DATA_FILE = f\"{DATA_DIR}/run42_sig_positions.npy\"\n",
    "BACKGROUND_DATA_FILE = f\"{DATA_DIR}/run42_bg_positions.npy\"\n",
    "\n",
    "max_barrel_radius = 86.3\n",
    "max_endcap_distance = 372.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.load(SIGNAL_DATA_FILE)\n",
    "background_data = np.load(BACKGROUND_DATA_FILE)\n",
    "\n",
    "background_data[background_data[:, :, 0] != -1, 0] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 0] + max_barrel_radius\n",
    ") / max_barrel_radius\n",
    "background_data[background_data[:, :, 0] != -1, 1] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 1] + max_barrel_radius\n",
    ") / max_barrel_radius\n",
    "background_data[background_data[:, :, 0] != -1, 2] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 2] + max_endcap_distance / 2\n",
    ") / max_endcap_distance\n",
    "\n",
    "signal_data[signal_data[:, :, 0] != -1, 0] /= max_barrel_radius\n",
    "signal_data[signal_data[:, :, 0] != -1, 1] /= max_barrel_radius\n",
    "signal_data[signal_data[:, :, 0] != -1, 2] /= max_endcap_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e81851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom data to cyclic coordinates\n",
    "r_bg = np.sqrt(background_data[:, :, 0] ** 2 + background_data[:, :, 1] ** 2)\n",
    "phi_bg = np.arctan2(background_data[:, :, 1], background_data[:, :, 0])\n",
    "z_bg = background_data[:, :, 2]\n",
    "background_data_cyclindric = np.stack([r_bg, phi_bg, z_bg], axis=-1)\n",
    "background_data_cyclindric[background_data[:, :, 0] == -1, :] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2310423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, latent_dim, kernel='rbf', sigma=1.0, weight=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "        self.weight = weight\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        z = y_pred  # shape: (batch_size, latent_dim)\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        prior = tf.random.normal(shape=(batch_size, self.latent_dim))  # standard Gaussian\n",
    "\n",
    "        return self.weight * self._mmd(z, prior)\n",
    "\n",
    "    def _mmd(self, x, y):\n",
    "        xx = self._compute_kernel(x, x)\n",
    "        yy = self._compute_kernel(y, y)\n",
    "        xy = self._compute_kernel(x, y)\n",
    "        return tf.reduce_mean(xx + yy - 2 * xy)\n",
    "\n",
    "    def _compute_kernel(self, x, y):\n",
    "        x = tf.expand_dims(x, 1)  # shape: (batch, 1, dim)\n",
    "        y = tf.expand_dims(y, 0)  # shape: (1, batch, dim)\n",
    "        dist = tf.reduce_sum((x - y) ** 2, axis=2)\n",
    "        return tf.exp(-dist / (2 * self.sigma ** 2))\n",
    "\n",
    "\n",
    "class SlicedWassersteinLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, latent_dim, num_projections=100, weight=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_projections = num_projections\n",
    "        self.weight = weight\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        z = y_pred  # shape: (batch_size, latent_dim)\n",
    "        batch_size = tf.shape(z)[0]\n",
    "\n",
    "        proj_vectors = tf.random.normal((self.num_projections, self.latent_dim))\n",
    "        proj_vectors = tf.math.l2_normalize(proj_vectors, axis=1)  # shape: (num_proj, latent_dim)\n",
    "\n",
    "        z_proj = tf.matmul(z, proj_vectors, transpose_b=True)  # shape: (batch, num_proj)\n",
    "        prior = tf.random.normal((batch_size, self.latent_dim))\n",
    "        prior_proj = tf.matmul(prior, proj_vectors, transpose_b=True)\n",
    "\n",
    "        z_sorted = tf.sort(z_proj, axis=0)\n",
    "        prior_sorted = tf.sort(prior_proj, axis=0)\n",
    "\n",
    "        return self.weight * tf.reduce_mean(tf.square(z_sorted - prior_sorted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0435d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLoss(keras.losses.Loss):\n",
    "    def __init__(self, latent_dim, diversity_encouragement=1, name=None):\n",
    "        super().__init__(name)\n",
    "        self.diversity_encouragement = diversity_encouragement\n",
    "        self.mmd_loss = MMDLoss(latent_dim=latent_dim, weight=1)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        ae_loss = tf.reduce_mean(tf.square(input - ae_output))\n",
    "\n",
    "        # include a regularization term to encourage diversity in the latent space to encourage the model to learn a variance of 1\n",
    "        if self.diversity_encouragement > 0:\n",
    "            diversity_loss = tf.reduce_sum(\n",
    "                tf.square(\n",
    "                    tf.ones(\n",
    "                        input.shape[-1],\n",
    "                    )\n",
    "                    - tf.math.reduce_variance(input, axis=0)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            diversity_loss = 0.0\n",
    "        return ae_loss + self.diversity_encouragement * diversity_loss \n",
    "\n",
    "\n",
    "class ReconstructionQuality(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"reconstruction_quality\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_loss = self.add_weight(name=\"total_loss\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        loss = tf.reduce_mean(tf.square(input - ae_output))\n",
    "        self.total_loss.assign_add(loss)\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_loss / self.count\n",
    "\n",
    "\n",
    "class FeatureVariance(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"feature_variance\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.variance = self.add_weight(name=\"variance\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        variance = tf.math.reduce_variance(input, axis=0)\n",
    "        self.variance.assign_add(tf.reduce_mean(variance))\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.variance / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87107e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 256\n",
    "feature_dim = 3\n",
    "hidden_dim = 6\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.components import (\n",
    "    SelfAttentionStack,\n",
    "    MLP,\n",
    "    PoolingAttentionBlock,\n",
    "    PointTransformerFromCoords,\n",
    "    DecoderQueries,\n",
    "    MultiHeadAttentionBlock,\n",
    ")\n",
    "from src.model.components import (\n",
    "    GenerateDecoderMask,\n",
    "    GenerateMask,\n",
    "    MaskOutput,\n",
    "    GetSequenceLength,\n",
    ")\n",
    "\n",
    "# Fixed size encoding models\n",
    "input = keras.Input(shape=(sequence_length, feature_dim), name=\"input\")\n",
    "mask = GenerateMask(name=\"generate_mask\")(input)\n",
    "sequence_length_layer = GetSequenceLength(name=\"get_sequence_length\")(mask)\n",
    "\n",
    "input_embedding = MLP(name=\"input_embedding\", output_dim=latent_dim, num_layers=3)(\n",
    "    input\n",
    ")\n",
    "\n",
    "attention_block = SelfAttentionStack(\n",
    "    name=\"self_attention_block\", num_heads=4, key_dim=latent_dim, stack_size=1\n",
    ")(input_embedding, mask)\n",
    "pooling = keras.layers.GlobalAveragePooling1D(name=\"global_average_pooling\")(attention_block, mask)\n",
    "\n",
    "if False:\n",
    "    attention_pooling = PoolingAttentionBlock(\n",
    "        name=\"pooling_attention_block\",\n",
    "        num_heads=8,\n",
    "        key_dim=hidden_dim,\n",
    "        dropout_rate=0.0,\n",
    "        num_seed_vectors=latent_dim,\n",
    "    )(attention_block, mask)\n",
    "fixed_size_encoding = keras.layers.Flatten(name=\"flatten\")(pooling)\n",
    "\n",
    "#fixed_size_encoding = MLP(name=\"mlp\", output_dim=1)(fixed_size_encoding)\n",
    "\n",
    "# Autoencoder model\n",
    "encoder = MLP(name=\"encoder\", output_dim=int(latent_dim / 1.5), num_layers=4)\n",
    "decoder = MLP(name=\"decoder\", output_dim=latent_dim, num_layers=4)\n",
    "\n",
    "\n",
    "output = keras.layers.Concatenate(name=\"concatenate\")([fixed_size_encoding, decoder(encoder(fixed_size_encoding))])\n",
    "\n",
    "# Define model\n",
    "autoencoder_layers = [encoder, decoder]\n",
    "# A: Embedding Model (input to fixed-size embedding)\n",
    "embedding_model = keras.Model(inputs=input, outputs=fixed_size_encoding, name=\"embedding_model\")\n",
    "\n",
    "# B: Autoencoder Model (fixed-size embedding to reconstructed latent vector)\n",
    "autoencoder_input = keras.Input(shape=(fixed_size_encoding.shape[-1],), name=\"ae_input\")\n",
    "encoded = encoder(autoencoder_input)\n",
    "decoded = decoder(encoded)\n",
    "ae_output = keras.layers.Concatenate(name=\"ae_concat\")([autoencoder_input, decoded])\n",
    "autoencoder_model = keras.Model(inputs=autoencoder_input, outputs=ae_output, name=\"autoencoder_model\")\n",
    "\n",
    "# C: Full Model (input to concatenated fixed + decoded vector)\n",
    "full_model = keras.Model(inputs=input, outputs=output, name=\"full_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f46de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = EncodingLoss(diversity_encouragement=2, latent_dim = latent_dim)\n",
    "metrics = [ReconstructionQuality(), FeatureVariance()]\n",
    "\n",
    "full_model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=loss_fn, metrics=metrics)\n",
    "autoencoder_model.compile(optimizer=keras.optimizers.Adam(1e-2), loss=EncodingLoss(diversity_encouragement = 0, latent_dim = latent_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a49269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 2/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3s/step - feature_variance: 0.0886 - loss: 107.5750 - reconstruction_quality: 0.2653\n",
      "\n",
      "Epoch 3/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 4/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 6s/step - feature_variance: 0.3062 - loss: 69.4411 - reconstruction_quality: 0.2238\n",
      "\n",
      "Epoch 5/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 6/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 5s/step - feature_variance: 0.6096 - loss: 31.6149 - reconstruction_quality: 0.3630\n",
      "\n",
      "Epoch 7/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 8/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - feature_variance: 0.8046 - loss: 9.0890 - reconstruction_quality: 0.4476\n",
      "\n",
      "Epoch 9/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 10/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 5s/step - feature_variance: 0.9063 - loss: 2.4861 - reconstruction_quality: 0.4883\n",
      "\n",
      "Epoch 11/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 12/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - feature_variance: 0.9642 - loss: 1.0067 - reconstruction_quality: 0.4808\n",
      "\n",
      "Epoch 13/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 14/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6s/step - feature_variance: 0.9891 - loss: 0.6432 - reconstruction_quality: 0.4745\n",
      "\n",
      "Epoch 15/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 16/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5s/step - feature_variance: 0.9985 - loss: 0.5499 - reconstruction_quality: 0.4690\n",
      "\n",
      "Epoch 17/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 18/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 6s/step - feature_variance: 1.0032 - loss: 0.4946 - reconstruction_quality: 0.4619\n",
      "\n",
      "Epoch 19/20\n",
      "➡️  Training only autoencoder\n",
      "\n",
      "Epoch 20/20\n",
      "🔁 Training full model\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 6s/step - feature_variance: 1.0030 - loss: 0.5274 - reconstruction_quality: 0.4518\n"
     ]
    }
   ],
   "source": [
    "x_train = background_data[:10000]\n",
    "epochs = 20\n",
    "autoencoder_train_every = 2\n",
    "autoencoder_train_steps = 100\n",
    "batch_size = 1024\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    if epoch % autoencoder_train_every == 0:\n",
    "        print(\"➡️  Training only autoencoder\")\n",
    "\n",
    "        # Step 1: Freeze input embedding and extract encodings\n",
    "        fixed_embeddings = embedding_model.predict(x_train, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        for i in range(autoencoder_train_steps):\n",
    "            # Step 2: Train only the autoencoder on these embeddings\n",
    "            autoencoder_model.fit(\n",
    "                x=fixed_embeddings,\n",
    "                y=fixed_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                epochs=1,\n",
    "                verbose=0\n",
    "            )\n",
    "    else:\n",
    "        print(\"🔁 Training full model\")\n",
    "        full_model.fit(\n",
    "            x=x_train,\n",
    "            y=x_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e954cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.2918582 , -0.5106309 ,  0.14294906,  1.7450398 , -0.5682388 ,\n",
       "         0.2878471 , -0.14214495, -0.26114702, -0.30662382,  0.29714516,\n",
       "        -1.1316632 ,  0.09863343,  1.3607783 ,  0.9319748 , -1.2696887 ,\n",
       "         0.5054124 ,  2.3402386 , -0.3812863 ,  0.18824877,  0.41202492,\n",
       "         1.1504692 , -1.0950174 ,  0.43162268,  0.6681028 , -0.96428645,\n",
       "         0.03551219,  0.34769547,  0.5842959 ,  2.9174266 , -1.0390228 ,\n",
       "         0.7652624 ,  0.17830877, -1.1441555 ,  0.60953224, -0.8939077 ,\n",
       "        -1.6499591 ,  0.556143  , -0.9901701 , -0.8074555 ,  2.7105122 ,\n",
       "         0.33631784,  0.07599775, -0.9133696 ,  0.2875812 ,  0.6705343 ,\n",
       "         0.12424276, -1.7945004 , -1.4842741 ,  0.26408294,  0.44468483,\n",
       "        -1.1845734 ,  0.46260887,  0.8979039 , -0.00468242,  0.45253587,\n",
       "        -0.6554974 ,  0.30323333,  0.6867631 , -2.0339274 , -0.47890267,\n",
       "        -0.06434529,  0.43443152,  0.7792276 ,  0.59222937],\n",
       "       [-0.4387758 , -0.00682363, -0.6070815 ,  2.256508  ,  0.20351833,\n",
       "        -0.58984053,  1.0871822 , -0.8579483 ,  0.80557185, -0.6931609 ,\n",
       "         0.01099435,  0.9058877 ,  1.9359373 , -0.6718298 , -0.16079675,\n",
       "        -0.4005788 ,  2.7321348 ,  0.8296279 , -0.82125354, -0.6911635 ,\n",
       "         1.9858824 , -0.35931683,  1.6118413 , -0.1696968 ,  0.01356569,\n",
       "         0.98826355, -0.72672135, -0.63165146,  2.800528  , -0.58181316,\n",
       "        -0.44786817,  1.3549832 , -0.40869698,  1.4183863 , -0.21401812,\n",
       "        -0.348525  , -0.25785047, -0.5056563 , -0.15785527,  1.6354136 ,\n",
       "        -0.68121177,  1.2625039 ,  0.08738882, -0.747498  , -0.7566042 ,\n",
       "        -1.2313733 , -0.22289027, -0.39347553, -0.3719808 , -0.3812676 ,\n",
       "        -0.3659137 , -0.6579697 , -0.19674776, -0.54015976, -0.46996483,\n",
       "         0.7057445 , -0.5769382 , -0.30104926, -0.9762994 ,  0.3921308 ,\n",
       "        -0.5922045 ,  1.2930392 , -0.42149174, -0.43203923],\n",
       "       [ 1.1742027 , -1.0111712 ,  1.0742769 , -0.6897445 , -1.093021  ,\n",
       "         1.2034377 , -1.0310901 ,  1.0527309 , -1.057176  ,  1.1408266 ,\n",
       "        -1.072053  , -0.9324231 , -0.9448784 ,  1.2052634 , -0.871235  ,\n",
       "         1.036303  , -0.20921369, -1.0222492 ,  0.96919113,  1.108877  ,\n",
       "        -0.89128876, -1.1117635 , -0.8821262 ,  1.0186601 , -1.000983  ,\n",
       "        -0.97090703,  1.0754017 ,  1.1914515 ,  0.5460273 , -0.9222148 ,\n",
       "         1.2002051 , -0.8721404 , -0.9863666 , -1.0487691 , -1.0158873 ,\n",
       "        -0.8780473 ,  1.3229584 , -0.99949765, -0.91187143,  1.2661324 ,\n",
       "         1.1028517 , -1.033815  , -0.8466128 ,  0.97327006,  1.1673182 ,\n",
       "         1.2252828 , -0.90182143, -0.9969448 ,  1.0084734 ,  1.2164643 ,\n",
       "        -0.9222791 ,  1.1577518 ,  0.97139955,  1.3552996 ,  1.1987708 ,\n",
       "        -1.0379479 ,  1.244307  ,  1.4492797 , -0.79495806, -0.99490243,\n",
       "         1.0727663 , -1.1589544 ,  1.1487342 ,  1.2838688 ],\n",
       "       [ 1.3309443 , -1.0338066 ,  1.1810795 , -0.94870704, -1.0221307 ,\n",
       "         1.2269375 , -1.1064324 ,  1.0897915 , -1.082509  ,  1.1372666 ,\n",
       "        -1.037112  , -0.9339972 , -0.9180632 ,  1.193535  , -0.796473  ,\n",
       "         1.0732738 , -0.53767014, -1.0310662 ,  1.0190369 ,  1.1775345 ,\n",
       "        -1.1168952 , -1.0852005 , -0.894321  ,  0.974581  , -0.9891605 ,\n",
       "        -1.0739099 ,  1.1943979 ,  1.1551342 ,  0.14818874, -0.85109234,\n",
       "         1.1708999 , -0.92199   , -0.93545294, -1.1445764 , -1.00481   ,\n",
       "        -0.7425651 ,  1.341122  , -0.93962955, -0.80955887,  0.954398  ,\n",
       "         1.0939621 , -1.0654455 , -0.7094986 ,  0.9593369 ,  1.1644877 ,\n",
       "         1.2370125 , -0.740952  , -0.86488074,  0.99946374,  1.2291734 ,\n",
       "        -0.8917543 ,  1.1300949 ,  0.8989211 ,  1.417477  ,  1.2284752 ,\n",
       "        -1.0089427 ,  1.2859932 ,  1.4741694 , -0.62148947, -1.030507  ,\n",
       "         1.2807872 , -1.1904663 ,  1.0898546 ,  1.2452714 ],\n",
       "       [-0.9957426 ,  1.1245189 , -1.1383711 ,  0.9993501 ,  1.1472539 ,\n",
       "        -0.9777584 ,  1.1469711 , -1.2307111 ,  1.1639891 , -1.0451313 ,\n",
       "         0.9992412 ,  1.2608654 ,  1.0515928 , -0.91156083,  1.1198047 ,\n",
       "        -1.0921133 ,  0.9581588 ,  1.0964297 , -1.2276456 , -1.02861   ,\n",
       "         1.029468  ,  0.91428304,  1.2062864 , -1.1313248 ,  1.0983461 ,\n",
       "         1.2594953 , -1.0998812 , -0.96134263,  0.45068315,  0.98850733,\n",
       "        -0.93175244,  1.1812688 ,  0.9845846 ,  1.1015565 ,  1.0391563 ,\n",
       "         0.961253  , -0.8759814 ,  1.0023944 ,  1.1513933 , -0.13593344,\n",
       "        -1.0594752 ,  1.1136459 ,  1.1704354 , -1.2002908 , -1.0276715 ,\n",
       "        -0.89367956,  0.91317105,  0.842816  , -1.1661667 , -1.025744  ,\n",
       "         0.96853334, -1.0480049 , -1.1477288 , -0.876423  , -0.9541874 ,\n",
       "         1.0987583 , -0.99918526, -0.7058807 ,  0.7418003 ,  1.1685225 ,\n",
       "        -1.0831219 ,  0.9477868 , -1.0131879 , -0.9317338 ],\n",
       "       [-1.040397  ,  1.2740848 , -1.1284902 ,  0.7051869 ,  1.1988437 ,\n",
       "        -0.9827975 ,  1.0427377 , -1.1673474 ,  1.1038084 , -1.025072  ,\n",
       "         1.0628173 ,  1.236234  ,  0.7683413 , -0.8619103 ,  1.2405937 ,\n",
       "        -1.1630876 ,  0.51872003,  1.0584178 , -1.2103158 , -1.0018775 ,\n",
       "         0.7813731 ,  1.1152159 ,  1.0690472 , -1.2418058 ,  1.1974279 ,\n",
       "         1.2137316 , -1.0889499 , -0.94901145, -0.00918521,  1.2741514 ,\n",
       "        -0.9574805 ,  1.0913222 ,  1.161494  ,  0.9457725 ,  1.2129537 ,\n",
       "         1.1104162 , -0.89057386,  1.2249438 ,  1.2635766 , -0.2925824 ,\n",
       "        -1.0199063 ,  0.9750378 ,  1.2384299 , -1.1654605 , -1.0016065 ,\n",
       "        -0.7480516 ,  1.0275924 ,  1.0360861 , -1.2274055 , -1.0533029 ,\n",
       "         1.1518152 , -1.0170286 , -1.187748  , -0.8535052 , -0.97494537,\n",
       "         1.0609944 , -0.98453104, -0.711074  ,  1.0059239 ,  1.203771  ,\n",
       "        -1.0955625 ,  0.8140895 , -0.96389633, -0.9000448 ],\n",
       "       [-1.0435729 ,  1.2137038 , -1.1622399 ,  0.63689584,  1.199147  ,\n",
       "        -0.9971591 ,  1.0712956 , -1.2192563 ,  1.0935063 , -0.9719697 ,\n",
       "         1.0802544 ,  1.2222819 ,  0.83165497, -0.8561114 ,  1.2733877 ,\n",
       "        -1.1462733 ,  0.58151627,  1.0886811 , -1.2291853 , -1.0019135 ,\n",
       "         0.7966462 ,  1.1175017 ,  1.0593454 , -1.1569245 ,  1.1684403 ,\n",
       "         1.261463  , -1.0174836 , -0.9084548 , -0.02711507,  1.1534107 ,\n",
       "        -0.9257817 ,  1.0587105 ,  1.1562545 ,  0.8498143 ,  1.2745768 ,\n",
       "         1.1475708 , -0.9080519 ,  1.1977111 ,  1.2729875 , -0.22025532,\n",
       "        -0.98949665,  1.0247399 ,  1.2626417 , -1.1656669 , -0.9994834 ,\n",
       "        -0.7441489 ,  1.0020757 ,  0.9416901 , -1.1947005 , -1.0751103 ,\n",
       "         1.0891228 , -0.9588233 , -1.2788781 , -0.9022424 , -0.9491506 ,\n",
       "         1.0634153 , -1.0014093 , -0.69203496,  0.9594766 ,  1.2541919 ,\n",
       "        -1.0513548 ,  0.8185779 , -1.0127922 , -0.94900304],\n",
       "       [ 0.09554262, -0.34706634,  0.05631216,  1.7970117 , -0.6832638 ,\n",
       "         0.01831422,  0.24927838, -0.08606891, -0.1841646 ,  0.19344418,\n",
       "        -0.9501098 ,  0.01195651,  1.3279266 ,  0.448216  , -1.1101414 ,\n",
       "         0.30767912,  2.5478592 , -0.23677416, -0.08875304, -0.01372049,\n",
       "         1.3748335 , -0.87062204,  0.7023583 ,  0.83658326, -0.77016646,\n",
       "         0.26371342, -0.11946348,  0.43788528,  3.2395024 , -0.9325189 ,\n",
       "         0.500161  ,  0.42843008, -0.99516356,  0.48354197, -0.69377136,\n",
       "        -1.5553019 ,  0.4174028 , -0.9225306 , -1.0933102 ,  2.739429  ,\n",
       "         0.29029754,  0.37251478, -1.1084497 ,  0.25223285,  0.30172876,\n",
       "         0.0082608 , -1.629693  , -1.5913687 ,  0.5746923 ,  0.27071533,\n",
       "        -1.0135454 ,  0.34818554,  0.73433864,  0.02089125,  0.32450148,\n",
       "        -0.432494  ,  0.18078399,  0.45756406, -1.9774257 , -0.30080733,\n",
       "         0.03527014,  0.5474296 ,  0.7072073 ,  0.5071729 ],\n",
       "       [ 0.2263101 , -0.56183904,  0.1891351 ,  1.6657542 , -0.46266595,\n",
       "         0.24586192, -0.03530746, -0.07240576, -0.3759373 ,  0.3294041 ,\n",
       "        -1.1357826 ,  0.06393652,  1.3939271 ,  0.9214324 , -1.2856325 ,\n",
       "         0.52992624,  2.4520323 , -0.51656127,  0.13598314,  0.5209521 ,\n",
       "         1.0108285 , -1.1188635 ,  0.30438146,  0.74771667, -0.9684805 ,\n",
       "        -0.00937341,  0.34930387,  0.58565235,  2.9455645 , -0.96450746,\n",
       "         0.832656  ,  0.24704741, -1.1512582 ,  0.4932125 , -0.9173856 ,\n",
       "        -1.6634369 ,  0.62650436, -0.9599353 , -0.9213254 ,  2.7820287 ,\n",
       "         0.47486272,  0.02101778, -1.0668135 ,  0.3843248 ,  0.6577031 ,\n",
       "         0.24133416, -1.8875651 , -1.6063023 ,  0.45353657,  0.45584667,\n",
       "        -1.2717049 ,  0.48624426,  0.86793983, -0.02318676,  0.48345307,\n",
       "        -0.70552385,  0.17302659,  0.6926763 , -2.033121  , -0.4982255 ,\n",
       "        -0.00660744,  0.34202057,  0.87331724,  0.59169805],\n",
       "       [-1.0370041 ,  1.3173777 , -1.0917265 ,  0.5790461 ,  1.1244144 ,\n",
       "        -0.9652463 ,  0.93923414, -1.1709566 ,  1.0032332 , -0.98946905,\n",
       "         1.1220042 ,  1.2210608 ,  0.52862054, -0.8021758 ,  1.2915026 ,\n",
       "        -1.212856  ,  0.31037992,  1.0605981 , -1.1928676 , -1.0061519 ,\n",
       "         0.6264467 ,  1.2679384 ,  0.974524  , -1.2842078 ,  1.2493515 ,\n",
       "         1.1582825 , -1.0789005 , -0.92189866, -0.2211428 ,  1.4116482 ,\n",
       "        -0.9327486 ,  1.0582399 ,  1.2678496 ,  0.86503774,  1.2983313 ,\n",
       "         1.1647055 , -0.89089537,  1.2946347 ,  1.3297055 , -0.3609439 ,\n",
       "        -0.9791387 ,  0.90976083,  1.2457858 , -1.2273225 , -0.9418887 ,\n",
       "        -0.64904153,  1.0834296 ,  1.1627425 , -1.2561377 , -1.0454342 ,\n",
       "         1.3107938 , -1.032929  , -1.1726233 , -0.7831729 , -0.9693142 ,\n",
       "         1.0084102 , -0.947446  , -0.6976199 ,  1.102096  ,  1.24707   ,\n",
       "        -1.0814452 ,  0.74241304, -0.9659829 , -0.9133199 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.predict(x_train[:10], batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158e98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mu3e_trigger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
