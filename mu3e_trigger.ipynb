{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4436aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dd0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"mu3e_trigger_data\"\n",
    "SIGNAL_DATA_FILE = f\"{DATA_DIR}/run42_sig_positions.npy\"\n",
    "BACKGROUND_DATA_FILE = f\"{DATA_DIR}/run42_bg_positions.npy\"\n",
    "\n",
    "max_barrel_radius = 86.3\n",
    "max_endcap_distance = 372.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.load(SIGNAL_DATA_FILE)\n",
    "background_data = np.load(BACKGROUND_DATA_FILE)\n",
    "\n",
    "background_data[background_data[:, :, 0] != -1, 0] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 0] + max_barrel_radius\n",
    ") / max_barrel_radius\n",
    "background_data[background_data[:, :, 0] != -1, 1] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 1] + max_barrel_radius\n",
    ") / max_barrel_radius\n",
    "background_data[background_data[:, :, 0] != -1, 2] = (\n",
    "    background_data[background_data[:, :, 0] != -1, 2] + max_endcap_distance / 2\n",
    ") / max_endcap_distance\n",
    "\n",
    "signal_data[signal_data[:, :, 0] != -1, 0] /= max_barrel_radius\n",
    "signal_data[signal_data[:, :, 0] != -1, 1] /= max_barrel_radius\n",
    "signal_data[signal_data[:, :, 0] != -1, 2] /= max_endcap_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e81851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom data to cyclic coordinates\n",
    "r_bg = np.sqrt(background_data[:, :, 0] ** 2 + background_data[:, :, 1] ** 2)\n",
    "phi_bg = np.arctan2(background_data[:, :, 1], background_data[:, :, 0])\n",
    "z_bg = background_data[:, :, 2]\n",
    "background_data_cyclindric = np.stack([r_bg, phi_bg, z_bg], axis=-1)\n",
    "background_data_cyclindric[background_data[:, :, 0] == -1, :] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87107e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 256\n",
    "feature_dim = 3\n",
    "hidden_dim = 6\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0435d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLoss(keras.losses.Loss):\n",
    "    def __init__(self, diversity_encouragement=1, name=None):\n",
    "        super().__init__(name)\n",
    "        self.diversity_encouragement = diversity_encouragement\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        ae_loss = tf.reduce_mean(tf.square(input - ae_output))\n",
    "\n",
    "        # include a regularization term to encourage diversity in the latent space to encourage the model to learn a variance of 1\n",
    "        diversity_loss = tf.reduce_sum(\n",
    "            tf.square(\n",
    "                tf.ones(\n",
    "                    ae_output.shape[-1],\n",
    "                )\n",
    "                - tf.math.reduce_variance(input, axis=0)\n",
    "            )\n",
    "        )\n",
    "        return ae_loss + self.diversity_encouragement * diversity_loss\n",
    "\n",
    "\n",
    "class ReconstructionQuality(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"reconstruction_quality\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_loss = self.add_weight(name=\"total_loss\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        loss = tf.reduce_mean(tf.square(input - ae_output))\n",
    "        self.total_loss.assign_add(loss)\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_loss / self.count\n",
    "\n",
    "\n",
    "class FeatureVariance(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"feature_variance\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.variance = self.add_weight(name=\"variance\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        latent_dim = y_pred.shape[-1] // 2\n",
    "        input, ae_output = tf.split(y_pred, [latent_dim, latent_dim], axis=-1)\n",
    "        variance = tf.math.reduce_variance(input, axis=0)\n",
    "        self.variance.assign_add(tf.reduce_mean(variance))\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.variance / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b6a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fixed_size_encoding_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fixed_size_encoding_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MLP</span>)               │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ generate_mask       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GenerateMask</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_blo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span> │ input_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionStac…</span> │                   │            │ generate_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pooling_attention_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,614</span> │ self_attention_b… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PoolingAttentionB…</span> │                   │            │ generate_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MLP</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span> │ pooling_attentio… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MLP</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,418</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MLP</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,450</span> │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │        \u001b[38;5;34m116\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mMLP\u001b[0m)               │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ generate_mask       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGenerateMask\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_blo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │      \u001b[38;5;34m4,104\u001b[0m │ input_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionStac…\u001b[0m │                   │            │ generate_mask[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pooling_attention_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m)     │      \u001b[38;5;34m1,614\u001b[0m │ self_attention_b… │\n",
       "│ (\u001b[38;5;33mPoolingAttentionB…\u001b[0m │                   │            │ generate_mask[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp (\u001b[38;5;33mMLP\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │         \u001b[38;5;34m54\u001b[0m │ pooling_attentio… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ mlp[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder (\u001b[38;5;33mMLP\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,418\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder (\u001b[38;5;33mMLP\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,450\u001b[0m │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ decoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,237</span> (39.99 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,237\u001b[0m (39.99 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,237</span> (39.99 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,237\u001b[0m (39.99 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.model.components import (\n",
    "    SelfAttentionStack,\n",
    "    MLP,\n",
    "    PoolingAttentionBlock,\n",
    "    PointTransformerFromCoords,\n",
    "    DecoderQueries,\n",
    "    MultiHeadAttentionBlock,\n",
    ")\n",
    "from src.model.components import (\n",
    "    GenerateDecoderMask,\n",
    "    GenerateMask,\n",
    "    MaskOutput,\n",
    "    GetSequenceLength,\n",
    ")\n",
    "\n",
    "# Fixed size encoding models\n",
    "input = keras.Input(shape=(sequence_length, feature_dim), name=\"input\")\n",
    "mask = GenerateMask(name=\"generate_mask\")(input)\n",
    "sequence_length_layer = GetSequenceLength(name=\"get_sequence_length\")(mask)\n",
    "\n",
    "input_embedding = MLP(name=\"input_embedding\", output_dim=hidden_dim, num_layers=3)(\n",
    "    input\n",
    ")\n",
    "\n",
    "attention_block = SelfAttentionStack(\n",
    "    name=\"self_attention_block\", num_heads=8, key_dim=hidden_dim, stack_size=3\n",
    ")(input_embedding, mask)\n",
    "# pooling = keras.layers.GlobalAveragePooling1D(name=\"global_average_pooling\")(attention_block, mask)\n",
    "\n",
    "attention_pooling = PoolingAttentionBlock(\n",
    "    name=\"pooling_attention_block\",\n",
    "    num_heads=8,\n",
    "    key_dim=hidden_dim,\n",
    "    dropout_rate=0.1,\n",
    "    num_seed_vectors=latent_dim,\n",
    ")(attention_block, mask)\n",
    "fixed_size_encoding = MLP(name=\"mlp\", output_dim=1)(attention_pooling)\n",
    "fixed_size_encoding = keras.layers.Flatten(name=\"flatten\")(fixed_size_encoding)\n",
    "\n",
    "# Autoencoder model\n",
    "encoder = MLP(name=\"encoder\", output_dim=int(latent_dim / 2), num_layers=4)(\n",
    "    fixed_size_encoding\n",
    ")\n",
    "decoder = MLP(name=\"decoder\", output_dim=latent_dim, num_layers=4)(encoder)\n",
    "\n",
    "output = keras.layers.Concatenate(name=\"concatenate\")([fixed_size_encoding, decoder])\n",
    "\n",
    "# Define model\n",
    "model = keras.Model(inputs=input, outputs=output, name=\"fixed_size_encoding_model\")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=EncodingLoss(diversity_encouragement=2),\n",
    "    metrics=[ReconstructionQuality(), FeatureVariance()],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a49269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 16s/step - feature_variance: 0.0813 - loss: 54.4903 - reconstruction_quality: 0.1942 - val_feature_variance: 0.2772 - val_loss: 34.0292 - val_reconstruction_quality: 0.5157\n",
      "Epoch 2/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 20s/step - feature_variance: 0.3579 - loss: 27.5135 - reconstruction_quality: 0.7045 - val_feature_variance: 0.6039 - val_loss: 11.0432 - val_reconstruction_quality: 0.9762\n",
      "Epoch 3/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 18s/step - feature_variance: 0.6127 - loss: 10.9696 - reconstruction_quality: 0.9764 - val_feature_variance: 0.9316 - val_loss: 1.4721 - val_reconstruction_quality: 1.1478\n",
      "Epoch 4/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 17s/step - feature_variance: 0.9129 - loss: 1.9198 - reconstruction_quality: 1.1167 - val_feature_variance: 1.1878 - val_loss: 3.4330 - val_reconstruction_quality: 1.1446\n",
      "Epoch 5/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 17s/step - feature_variance: 1.0517 - loss: 1.3273 - reconstruction_quality: 0.9703 - val_feature_variance: 1.0082 - val_loss: 0.7173 - val_reconstruction_quality: 0.6210\n",
      "Epoch 6/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 16s/step - feature_variance: 0.9725 - loss: 0.8329 - reconstruction_quality: 0.6473 - val_feature_variance: 1.0979 - val_loss: 1.2901 - val_reconstruction_quality: 0.6117\n",
      "Epoch 7/30\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 14s/step - feature_variance: 1.0104 - loss: 0.7418 - reconstruction_quality: 0.6089"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = background_data[:10000]\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x=train_data,\n",
    "    y=np.concatenate([train_data, train_data], axis=-1),\n",
    "    batch_size=1024,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3433c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mu3e_trigger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
